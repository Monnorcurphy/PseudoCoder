area of a polygon

Formula to find the area with points A, B, and C (let’s assume C is the origin):

(X coordinate of A * Y coordinate of B - Y coordinate of A * X coordinate of B)
/2

Make sure you take the absolute value, since it can be negative in certain cases




going through a graph


You can go through a graph in pre, in, or post order. What does that mean?

For pre order, you go from the root, to the left node, to the right node.

For in order you go from the left, to the root, to the right node.

For post order you go from the left, to the right, to the root.

All three of these traversal methods are DFS (Depth First Search)

You don't need to memoize when you are going through a tree because there is only
one path to any given node. If there is more than one path to a node, that means
there is a cycle, and it is therefore a graph.


DFS should be written recursively, BFS should be written iteratively.



Answers for google stuff

What is the difference between top down and bottom up?

Top down- you start at the top and break the problem down until you are at the base level. Top down cannot be coded until the whole problem is understood, but this gives the problem a clear direction.

Memoization is a key feature of the top down approach. You track the work that you have already done, and cache those values. If you have a cached value, then you use the pre computer answer.

Bottom up- you start with small examples and build these examples up into larger and larger parts. You can begin programming bottom up problems immediately, but can cause a problem if you don’t know how the problems are going to fit together.

Dynamic programming is the process of picking what order your computations will go in, and working these values into the future of the algorithm.


Show me the formula to determine a team of 11 people, out of 14 choices?

If you have a team of 14 people, with only 11 open spots, how many possibilities are there?

n!/ k!(n -k)!

Why? What does that mean? Go through it one by one.

In the first choice you have n options, then n -1, and so on.

After we have selected a number of players, we have n - i choices left.

What is that n-k! doing on the bottom? Order does not matter, unlike in other problems. Because a team with Johnson and Briggs, is the same as a team with Briggs and Johnson.


What is a Process? How do multitasking, threads work?

A process is a computer program that is being executed. Depending on OS, a process can have multiple threads of execution, some of which are executed concurrently.

Multitasking allows process to share processors (CPUs) and other resources. Each CPU executes a task, but multitasking allows processes to share processors. When a processor is handling multiple processes, it is switching.

Switches happen when tasks preform input/output operations*, a task indicates it can be switched, or hardware interrupts*.

Input/output (I/O)- the communication between an information processing system (i.e.- computer) and the outside world (a human, or other processing system). Input- signals or data received by the system, output are the signals or data sent out.  Input devices - keyboard, mouse
Output devices- Monitors, printers

Hardware interrupts- a signal to the processor indicating an event needs immediate attention. The processor responds by saving state and executing an interrupt handler. interrupts can be either hardware or software interrupts.

Hardware interrupts- Can be things like key presses, or mouse moving. Hardware interrupts are asynchronous and can occur in the middle of an execution.

Software interrupt- Either exceptional conditions (like diving by 0) or a special instruction which causes an interrupt when it is executed (some condition is met and the process ends).

A common form of multitasking is time-sharing which is rapid context switches making it seem like multiple processes are being executed at the same time on the processor. 
The OS holds the information about the active processes in process control blocks*. The resources are usually associated with threads*.

Process Control Block is a data structure in the operating system which holds information needed to manage a particular process.

Thread the smallest sequence of programmed instructions that be managed independently by a scheduler. In most cases threads are part of processes.

Threads generally share process state, memory, and other resources. Context switching is faster between threads rather than processes.

Single threading is when you process one command at a time. Multithreading means threads share resources within the context of a process.

Multithreads are more responsive, since separate threads can handle different execution tasks, faster execution, since threads can use parallel execution, better system utilization, since one thread can access the cache while another accesses external storage.  
Some of the drawbacks are synchronization, programmers must remember that the address space is shared between threads and ensure there is not a race condition. Additionally, threads can perform an illegal operation which may crash the system and disrupt the processing.

Threads are typically subsets of a process, carry less state information, share memory and other resources, share address spaces, and have faster context switching over processes.

Processes have individual address spaces, carry more state information, only communicate through inter-process communication mechanisms*

An address space is range of addresses which may correspond to memory cell, or other logical or physical entity.

A processor can only process one task at a time, unless it has multiple cores. In that case you can handle multithreading.


What are the process states?

First- created. The process scheduler assigns it the waiting state

Second- The process is waiting, and waits until the scheduler does a context switch and load the process into the processor

Third- the process becomes running, with the processor executing the process instructions.

Third.5 - If the process needs to wait for a resource it is blocked, and waits until it gets the input (a file, user input, etc.)

Fourth- Once the process has finished, it is terminated by the operating system.

What is a context switch? How do you determine when to switch? How does user and kernel mode switching factor in? And how does busy waiting factor in?

The process of storing and restoring the state of a process or thread. This enables multiple processes to share a single CPU, and is essential to multitasking operating system.

Multitasking- Schedulers determine when processes switch. Pre-emptive multitasking has a timer, which ensures that processes switch and share CPU time. This works by having a timer, and the scheduler switches the process after the timer runs out.

Interrupt Handling- If the CPU needs some data (from a file, a user input, etc.) then it can wait for that data to be retrieved, and do other things while it waits for the data. Then, a program called an interrupt handler, and the CPU is interrupted.

Transitions between user mode and kernel mode a context switch is not necessary. This is called a mode transition, and is not a context switcher, however, some Operating Systems may require a context switch at this point.

Executing can depend on the priority of the thread or process, in that case this is called a priority queue.

Busy waiting (busy looping, or spinning) is telling a CPU to wait to execute, until something that program is waiting on is available. It is generally considered an anti-pattern, since instead of waiting, the CPU can do other tasks while it is waiting.

What is a lock/mutex? How does it work?  A lock, or mutual exclusion, is a way to limit access to a resource when there are multiple threads of execution.

Generally, a process has a lock and executes. Other processes which need resources may request the lock. If the thread waits (spins), then it is called a spin lock. Spin locks are only efficient if the lock is held for a short period of time, since this allows the os not to re-scheduling, which can be expensive. This is a bad practice if the lock is held for a long time or if the thread holding the lock needs something from the locked thread (deadlock)

In is important to have a standard for acquiring the lock, so that you avoid live lock and dead lock.

Lock overhead- the resources used for initializing a lock, memory space, CPU time to initialize and destroy locks, time for acquiring and releasing locks.

Lock Contention- A process or thread tries to acquire a lock held by another. You can avoid this by making locks more fine grained (locking a row rather than a table, or locking a cell rather than a row).

deadlock- at least two tasks are waiting for a lock that the other task holds. if nothing changes, this will cause the process to wait forever.

Coarse lock granularity (a small number of locks, each protecting large segments of data) results in less lock overhead, but worse performance when processes run concurrently (more lock contention).   Fine lock granularity increases the overhead of the locks, but reduces the lock contention.

Pessimistic locking- If someone wants to update some data, then no one else can touch the data until the current user is finished. This can cause long waits. This should be used when the cost of protecting data through locks is less than the cost of rolling back transactions. It is not appropriate for Web application development.

Optimistic locking- Is where users update concurrently, and if the information is incorrect then some updates will fail. This can be a bad implementation when many users are going to repeatedly update a piece of information, causing many users to get failed updates.

A race condition, or a race hazard, is when two things are operating at the same time even though a programmer did not intend them to. One process was supposed to wait on one of the other ones, and now they are both trying to finish meaning the data will be overwritten by whichever process finishes first, even though they were supposed to finish concurrently.

Mutual exclusion requires that one thread of executing never enters its critical section at the same time another concurrent thread enters its own critical section.

A critical section is a shared resource between programs. It cannot be executed by more than one process or program, and examples include a data structure, a peripheral device, or a network connection that would not work if operated by concurrent access.

Lock free versus wait free
An algorithm in which at least a single thread can progress all the time while other threads might starve is lock free.

Wait free- An algorithm which promises that all the threads can progress all the time.

Obstruction free is an algorithm that promises any thread can progress if executed in isolation.

What is deadlock? Live lock?

A deadlock is when two, or more, locked resources are required by two different threads or locks. Both need the others resources to continue, and cannot move until the resources are relinquished.

Dead lock requires four conditions:
	First - (Mutual Exclusion) Only one process/thread can use a resource at a time
	Second - (Hold and wait) a process is currently holding at least one resource and is requesting other resources which are being held by other processes.
	Third - (No preemption) a resource can only be related voluntarily by the process holding it.
	Fourth - (Circular wait) a process is waiting for a resource held by another process.

Most ways to avoid deadlock work by preventing the fourth condition. Some large ways to avoid deadlock

Detection- deadlocks can occur, and when the system detects and restarts one or more processes. These corrections can look like process termination or resource preemption.

	Process termination is aborting one or more processes, which can be expensive since some data has to be recalculated. You can choose to abort one process at a time until deadlock is resolved, but this means the system must continually recalculate if deadlock is still in existence (expensive).

	Resource preemption- resources can be preempted until the deadlock is broken.

Prevention- Remove the mutual exclusion component (impossible in resources that cannot be spooled*). Hold and wait resource can be resolved by processes having all resources requested at once, however this can be inefficient since they may not need resources for a while, and this results in holding resources for a long time.

(Spooling is when a process must communicate with a slow peripheral (like a printer) and allows the program to hand off work to be done and then proceed to other tasks. The spooler maintains an orderly sequence of jobs and feeds data at a rate the peripheral can handle.

Conversely for slow input (i.e. a card reader), the spooler can do batch processing, where it waits to do jobs until the input information is available. )

No preemption, may not work since it may cause thrashing or rollbacks which can be expensive. Finally, the circular wait condition involves disabling interrupts during critical sections and use a hierarchy to determine the ordering of resources.

Thrashing is when a computer is rapidly exchanging data between data on the disk an data in memory (also known as paging). This can cause performance to degrade or collapse.

Livelock is like deadlock except the processes required are constantly changing. Livelock is often an algorithm attempting to resolve deadlock, but not doing it properly. Livelock can be avoided by ensuring that one process takes action.


What is a sempahore?

A semaphore is an ADT (abstract data type) used to control access to a common resource by multiple processes in a concurrent system. A trivial semaphore is a variable which has a changed value, which allows or disallows access to that resource.

They are useful in preventing race conditions. Sempahores can be counting (i.e. there is a certain amount of a resource allowed, and the semaphore tracks how much is being used) and binary (1 and 0, used or unused, locked or unlocked).

Programmers must be careful of requesting a resource, but forgetting to release it, releasing a resource that was not requested, holding a resource for a long time without using it, using a resource without requesting it (or after releasing it).


Translation Lookaside buffer

TLB is a memory cache that is used to reduce the time taken to access a user memory location. If a user is looking for something, then the TLB will be faster at finding it.

When an address space switch happens, the TLB may hold information that is irrelevant, so the computer may flush the TLB, meaning it will be empty. This operation is not the most efficient, so newer CPUS mark which process an entry is for.



What is concurrency?

Concurrency is the property of a program being able to be broken up into separate partially ordered units. Meaning, even if portions of a program are executed out of order, the result will be the same.


What is a monitor?

A monitor consists of a mutex (lock) and condition variables. Condition variables are a container of threads that are waiting for a certain condition. Monitors provide a mechanism for threads to give up exclusive access in order to wait for some condition to be met.


What is system design?
A system is the creation, defining, and development of systems in order to satisfy user requirements. Software is standardized which means systems can now be modular, and this increases the importance of software engineering.

Unified Modeling Language is the standard language which is supposed to provide a way to visualize the design of a system. The idea of UML is to blueprint elements such as activities (jobs), components of the system (how they interact with software components), how the system will run, how components and interfaces interact, and external user interface.

UML have static (structural) views which emphasize the static structure of the system using objects, attributers, operations, and relationships. Including class diagrams and composite structure diagrams.

Used to document the software architecture of software systems, for example the component diagram describes how a software system is split up into components and shows the dependencies amongst these components.

It also has dynamic (behavioral) view which emphasizes the dynamic behavior of the system through displaying the internal states of objects.

Used to describe how software/ideas are supposed to function and interact with each other.

Logical design pertains to the abstract idea of how the data flows, inputs/outputs to the system, etc.   The physical design describes how data is actually input into the system, and is usually broken down into user interface, data design, and process design.   User interface design is concerned with how users add information to the system, and how it presents that information back to them.

Data design is concerned with how the data is represented and stored within the system.

Process design is concerned with how data moves through the system, how/where it is validated, etc.


What is an interface?

Interfaces are places where two components of technology communicate with non related components. A hardware interface is usually defined by the mechanical, electrical, and logical signals.

Software interfaces are more general, and encompass more parts. A software interface could refer to an operating system, which interfaces with the hardware. Applications and programs can interface with streams*. Object oriented programs interacts with methods.

A stream is a sequence of data elements made available over time.

Software interfaces should be designed such that access is not public automatically, access should be defined through access points.

class hierarchy
Classification of object types, where objects can be related to each other by building off of one another. You could have an animal class, which would have many different sub-classes that were related (i.e rhinos, wolves, and dolphins), but had their own distinct characteristics. However, there would be some common thread amongst these, like eating, moving, making noises, etc.

polymorphism

The idea that a single interface can handle multiple different inputs.
For example, the + sign in Python can handle adding numbers together, but it can also add strings together.

The ability to appear in many forms. In programming this refers to a program’s ability to handle different objects differently, depending not their class or data type.

Distributed computing

Components located on connected (networked) computers communicate and coordinate their actions by passing messages. These components interact to achieve a common goal. Significant characteristics include concurrency of components, lack of a global clock, and independent failure of components.

Distributed program is a distributed system that is run by a computer program. In distributed programming it is common for tasks to be partitioned out  and solved by one, or more, computers.

Distributed used to mean computers that were spread out in a geographic area, but today can mean autonomous processes that run on the same computer, and interact with each other by message passing.

Concurrent computing, parallel computing and distributed computing have a lot of overlap and no clear distinction. A system could be described as parallel and distributed. Distributed computing is a loosely coupled form of parallel computing (parallel computing- tight coupling of distributed).

Parallel computing- all processors have access to a shared memory to exchange information.

Distributed computing- each processor has its own private memory, information is exchanged by passing messages between processors.

Architectures- still distributed computing
Low level- CPUs are connected via some sort of network, be it printed onto a circuit board or made up of loosely coupled devices.

Higher level- A communication system must interconnect these processes running on those CPUs.

Distributed programming falls into some of these categories:

Client-server- clients contact the server for data then format and display int to the users. Permanent changes are committed back to the server.

In this model work is done by a server, and is requested by a client. The server does the work and the client initiates the contact. Examples email, network printing, the world wide web.

Three-tier- Most web applications. Architectures that move the client intelligence to a middle client. Simplifies application deployment.

N-tier- refer to web applications which further forward their requests to enterprise services. This lead to the success of application servers.

multi-tier architecture - client server architecture presentation, application processing, and data management functions are physically separated. Three tier is the most common.

Example- web development such as e-commerce website. A front end web server which delivers content. A middle content processing and generation level application server (Rails, Django, Flask). Then a backend database or data store, made up of data sets and database management systems software, which provides access to the data.

Peer to Peer- no special machines that provide a service or manage the network resources. Everyone is considered equal in doing the task. Uses include file sharing, instant and voice communication.

Example-  Skype, Napster

Distributed systems are more reliable. Many low end computers provide a lower chance of failure than one high end computer.


Scaling out vs scaling up

Multi-core concurrency is referred to as Scale up, and distributed computing is referred to as scale out.

Scaling up is the process of making a computer more and more workload, while scaling out is getting more computers to handle the problem.   Scaling up is cheaper, but is more risky.


SQL vs NoSQL:
Sql is a relational database


How the internet works

Everything that accesses the internet has an Internet Protocol address. You connect to the internet through an ISP, which gives you an IP address that is unique.

TCP/IP protocol stack. Your signal goes from an application, to the TCP (directs packets to a specific application on a computer), to the IP (directs packets to a specific computer using an IP address) which goes to the hardware (converts binary packet data to network signals…and back).

Big enough messages and data are broken up into packets.

Your computer sends the signal down the chain until it is sent to your ISPs router, examines the destination and sends it to the appropriate location. Then it goes back up the stack (bottom to top) until the message reaches your application.

Internet infrastructure- There are networks which make up the backbone of the internet (Network Service Providers, NSPs). These NSPs are required to connect to Network Access Points (NAPs). At the NAPs your packets may jump from one NSP to another. There are also Metropolitan Area Exchanges (MAEs) which are like NAPs but are privately owned. Both NAPs and MAEs are referred to as Intent Exchange Points (IXs). NSPs sell band with to smaller networks (ISPs).

Your packets are sent to a router, which looks to see if it knows where the IP address is, if it does not it will send it up a chain of routers until it reaches the NSP backbone, and then back down the chain.

Domain Name Service is a distributed database which keeps track of computer’s names and their IP addresses. DNS servers hold portions of the database, and if they don’t know where to find the IP address you are looking for, you will be again sent up a hierarchy of DNS servers until it is found.

HTTP and the WWW- HTTP is the protocol that web browsers and servers use to communicate with one another, not to be confused with HTML which is used to write web pages. When you type a url into a page, you ping the DNS server, it finds the page, requests something from the web server (a continually connected computer). The web server responds, or you get 404ed.

TCP receives packets, and makes sure they are in the correct order, IP does not do this. It just sends things wherever, and in any order.










  
